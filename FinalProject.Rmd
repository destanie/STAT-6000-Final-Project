---
title: "My Document Title"
author: "Your Name"
date: "`r Sys.Date()`"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



### Loading Libraries
```{r}
library(ISLR2)
library(ggcorrplot)
library(corrplot)
library(leaps)
```


### Analysis
```{r}
#loading data
bananas <- read.csv("C:\\Users\\cww0035\\OneDrive - Auburn University\\Documents\\GitHub\\STAT-6000-Final-Project\\going_bananas.csv")

#confirming that there are no holes in the dataset
bananas <- na.omit(bananas)
View(bananas)

#converting categorical variables to factors
bananas$ripeness_category <- as.factor(bananas$ripeness_category)
bananas$quality_category <- as.factor(bananas$quality_category)
bananas$region <- as.factor(bananas$region)
bananas$variety <- as.factor(bananas$variety)
summary(bananas)
str(bananas)

#creating subset for correlations and observing relationships
#the correlation subset omits variety, region, quality_category, ripeness_category, and harvest_date
cor_subset <- bananas[,c(1,4,6,8,9,10,11,13,14,15,16)]
cor(cor_subset)
corrplot(cor(cor_subset))
ggcorrplot(cor(cor_subset))

#quality category is a categorical variable based on the quality score, so we decided to remove it from the analysis.

#the three continuous variables that have the strongest relationship with quality score are
#length, sugarcontent, and ripeness index

#we converted the harvest_date to a binary variable based on the month of harvest, September = 0, October = 1. There were only two months in the harvest window.
```

### Subset and Base MLR
```{r}
#creating subset of variables selected for analysis.  This excludes 2 categorical variables that had several categories each.  It also excludes the sample ID, quality_category, and the harvest_date on which harvest_binary is based.
banana_sub <- bananas[,c(4,6,7,8,9,10,11,13,14,15,16,17)]
View(banana_sub)

attach(banana_sub)
model <- lm(quality_score ~ ., data = banana_sub)
summary(model)

#adjusted R squared is 98.3%, but only 4 variables are significant contributors to the model.


```
### Training and Test Sets
```{r}
set.seed(123)
train <- sample(1:nrow(banana_sub), nrow(banana_sub)*.7)
test <- setdiff(1:nrow(banana_sub), train)

```


### Best Subset Selection
```{r}
#performing best subset selection
regfit.best <- regsubsets(quality_score ~ ., data = banana_sub[train,], nvmax = 11)

#creating a model matrix from the test data
test.mat <- model.matrix(quality_score ~ ., data = banana_sub[test,])

predict.regsubsets <- function (object ,newdata ,id ,...){
form=as.formula (object$call [[2]])
mat=model.matrix (form ,newdata )
coefi =coef(object ,id=id)
xvars =names (coefi )
mat[,xvars ]%*% coefi
}


##choosing among models of different sizes using cross-validation
k <- 10
n <- nrow(banana_sub)
set.seed(123)
folds <- sample(rep(1:k, length = n))
cv.errors <- matrix(NA, k, 11, dimnames = list(NULL, paste(1:11)))
##writing a for loop that performs cross-validation
for(j in 1:k) {
  best.fit <- regsubsets(quality_score ~ ., data = banana_sub[folds != j, ], nvmax = 11)
  for (i in 1:11) {
    pred = predict(best.fit, banana_sub[folds == j, ], id = i)
    cv.errors[j, i] = mean((banana_sub$quality_score[folds == j]- pred)^2)
  }
}

#obtaining errors for each of the cross-validation models
#errors drop substantially at 3 variables and then don't drop much after that
mean.cv.errors <- apply(cv.errors, 2, mean)
mean.cv.errors

#viewing plot to see that CV selects an 3-variable model
par(mfrow = c(1,1))
plot(mean.cv.errors, type = "b")

#running best subset selection on the full dataset and pulling coefficients for the 3-variable model
reg.best <- regsubsets(quality_score ~ ., data = banana_sub, nvmax = 11)
coef(reg.best, 3)

#plugging 3 feature model into MLR
model_best <- lm(banana_sub$quality_score ~ banana_sub$ripeness_index + banana_sub$sugar_content_brix + banana_sub$length_cm, data = banana_sub)
summary(model_best)
```

###Forward and backward stepwise selection
```{r}
#load data
bananas <- read.csv("C:\\Users\\mecal\\OneDrive - Auburn University\\Stat\\going_bananas.csv")
library(caret)

#forward stepwise selection model
regfit.fwd <- regsubsets(quality_score ~ ., data = banana_sub[train,], nvmax = 11, method = "forward")
summary(regfit.fwd)

#create model matrix
test.mat.step <- model.matrix(quality_score ~ ., data = banana_sub[test , ])
coef(regfit.fwd,11)

#predict function
predict.regsubsets <- function (object ,newdata ,id ,...){
form=as.formula (object$call [[2]])
mat=model.matrix (form ,newdata )
coefi =coef(object ,id=id)
xvars =names (coefi )
mat[,xvars ]%*% coefi
}

#name vectors and store results
k <- 10
n <- nrow(banana_sub)
set.seed(123)
folds <- sample(rep(1:k, length = n))
cv.errors <- matrix(NA, k, 11, dimnames = list(NULL, paste(1:11)))

#for loop with predict function
for(j in 1:k) {
  best.fit <- regsubsets(quality_score ~ ., data = banana_sub[folds != j, ], nvmax = 11, method = "forward")
  for (i in 1:11) {
    pred = predict(best.fit, banana_sub[folds == j, ], id = i)
    cv.errors[j, i] = mean((banana_sub$quality_score[folds == j]- pred)^2)
  }
}

#obtaining errors for each of the cross-validation models
mean.cv.errors <- apply(cv.errors, 2, mean)
mean.cv.errors

#viewing plot
#cv selects 3-variable model
par(mfrow = c(1,1))
plot(mean.cv.errors, type = "b")

#running best subset selection on the full dataset and pulling coefficients for the 3-variable model
reg.best.fwd <- regsubsets(quality_score ~ ., data = banana_sub, nvmax = 11, method = "forward")
coef(reg.best.fwd, 3)
```

```{r}
#backward stepwise selection
regfit.bwd = regsubsets(quality_score ~ ., data = banana_sub[train,], nvmax=11, method = "backward")
summary(regfit.bwd)
coef(regfit.bwd,11)

#create model matrix
test.mat.step <- model.matrix(quality_score ~ ., data = banana_sub[test , ])
coef(regfit.bwd,11)

#predict function
predict.regsubsets <- function (object ,newdata ,id ,...){
form=as.formula (object$call [[2]])
mat=model.matrix (form ,newdata )
coefi =coef(object ,id=id)
xvars =names (coefi )
mat[,xvars ]%*% coefi
}

#name vectors and store results
k <- 10
n <- nrow(banana_sub)
set.seed(123)
folds <- sample(rep(1:k, length = n))
cv.errors <- matrix(NA, k, 11, dimnames = list(NULL, paste(1:11)))

#for loop with predict function
for(j in 1:k) {
  best.fit <- regsubsets(quality_score ~ ., data = banana_sub[folds != j, ], nvmax = 11, method = "backward")
  for (i in 1:11) {
    pred = predict(best.fit, banana_sub[folds == j, ], id = i)
    cv.errors[j, i] = mean((banana_sub$quality_score[folds == j]- pred)^2)
  }
}

#obtaining errors for each of the cross-validation models
mean.cv.errors <- apply(cv.errors, 2, mean)
mean.cv.errors

#viewing plot
#cv selects 3-variable model
par(mfrow = c(1,1))
plot(mean.cv.errors, type = "b")

#running best subset selection on the full dataset and pulling coefficients for the 3-variable model
reg.best.bwd <- regsubsets(quality_score ~ ., data = banana_sub, nvmax = 11, method = "backward")
coef(reg.best.fwd, 3)
```


 
## lasso logistic regression (classification)
```{r}
# loading data
set.seed(123)
bananas <- read.csv("going_bananas.csv")
# y is going to be our target variable which is the quality_category
y <- bananas$quality_category 
y <- as.factor(y)  # making sure y is a factor
X <- model.matrix(~ . - quality_category - 1, data = bananas)  
# creating the predictor matrix (X) and excluding the response variable

# loading the library we need
library(glmnet)
# fitting Lasso logistic regression
lasso_model <- glmnet(X, y, alpha = 1, family = "multinomial") 

# performing cross-validation
cv_lasso <- cv.glmnet(X, y, alpha = 1, family = "multinomial")

# getting and printing the best lambda
best_lambda <- cv_lasso$lambda.min
cat("Optimal Lambda:", best_lambda, "\n")

# plotting cross-validation results
plot(cv_lasso)

# coefficients for the optimal lambda
lasso_coefficients <- coef(lasso_model, s = best_lambda)
print(lasso_coefficients)

# predicting probabilities
probabilities <- predict(lasso_model, newx = X, s = best_lambda, type = "response")

# predict class labels
predicted_classes <- predict(lasso_model, newx = X, s = best_lambda, type = "class")

# viewing predictions
head(predicted_classes)

# creating a confusion matrix
# Create confusion matrix
#install.packages("caret")
library(caret)
confusionMatrix(as.factor(predicted_classes), as.factor(y))

```

## lasso logistic regression results

The confusion matrix shows that 434 instances of the "Good" category were correctly classified, with no misclassifications in the off-diagonal entries, indicating **perfect** classification by the model. The confidence interval for the model's performance is [0.9963, 1], demonstrating extremely high confidence. It is worth noting that the "Processing" category constitutes the majority class. Additionally, the small p-value suggests that the model's accuracy is significantly better than random guessing. However, the perfect classification raises concerns about potential overfitting, which may overshadow the key insights provided by the model.. 

## Adressing Overfitting

```{r}
# Splitting the data into training and test sets (70/30 split)
set.seed(123)
train_indices <- sample(1:nrow(X), size = 0.7 * nrow(X))
X_train <- X[train_indices, ]
y_train <- y[train_indices]
X_test <- X[-train_indices, ]
y_test <- y[-train_indices]

# fitting thr Lasso model on training data
lasso_model <- glmnet(X_train, y_train, alpha = 1, family = "multinomial")

# Performing cross-validation on the training data
cv_lasso <- cv.glmnet(X_train, y_train, alpha = 1, family = "multinomial")

# best lambda
best_lambda <- cv_lasso$lambda.min
cat("Optimal Lambda:", best_lambda, "\n")


# prediction of probabilities and classes on test data
probabilities_test <- predict(lasso_model, newx = X_test, s = best_lambda, type = "response")
predicted_classes_test <- predict(lasso_model, newx = X_test, s = best_lambda, type = "class")

# confusion matrix for test set
confusionMatrix(as.factor(predicted_classes_test), as.factor(y_test))

```


## Intepreting

After addressing the overfitting issue, the model is no longer perfect. It performs well in predicting the "Good" category, and both "Processing" and "Unripe" are accurately classified. However, the model struggles with the "Premium" classification. Despite achieving a high overall accuracy and a strong Kappa statistic of 0.95, the difficulty in predicting the "Premium" class comes from the limited number of premium samples in the dataset, thus highlighting the model's challenges with minority class prediction. Nonetheless, this model is an improvement over the earlier "perfect" model, as it provides a more realistic and robust representation of the data. 
 
## Modeling and Visulization

## Confusion matrix heatmap (Perfect Model)
```{r}
# Confusion Matrix Heatmap
library(ggplot2)
library(reshape2)

# Create the confusion matrix from the perfect modeling
conf_matrix <- matrix(c(434, 0, 0, 0, 
                        0, 25, 0, 0, 
                        0, 0, 506, 0, 
                        0, 0, 0, 35), 
                      nrow = 4, byrow = TRUE)

rownames(conf_matrix) <- c("Good", "Premium", "Processing", "Unripe")
colnames(conf_matrix) <- c("Good", "Premium", "Processing", "Unripe")

# Melting matrix for ggplot
conf_melted <- melt(conf_matrix)

# Plotting the heatmap
ggplot(data = conf_melted, aes(x = Var2, y = Var1, fill = value)) +
  geom_tile(color = "white") +
  geom_text(aes(label = value), color = "black") +
  scale_fill_gradient(low = "white", high = "lightblue") +
  labs(title = "Confusion Matrix Heatmap", x = "Predicted", y = "Actual") +
  theme_minimal()

```

## Confusion Matrix Heatmap (2nd Model)

```{r}
# Load required libraries
library(ggplot2)
library(reshape2)
library(caret)

# Define confusion matrix
conf_matrix <- confusionMatrix(as.factor(predicted_classes_test), as.factor(y_test))

# Extract confusion matrix table
conf_table <- as.matrix(conf_matrix$table)

# Melt the confusion matrix table for ggplot
conf_melted <- melt(conf_table)

# Plot confusion matrix heatmap
ggplot(data = conf_melted, aes(x = Reference, y = Prediction, fill = value)) +
  geom_tile(color = "white") +
  geom_text(aes(label = value), color = "black", size = 4) + # Add counts as text
  scale_fill_gradient(low = "white", high = "lightblue") +
  labs(title = "Confusion Matrix Heatmap", x = "Actual", y = "Predicted", fill = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```


## Showing feature importance

```{r}
# library
library(ggplot2)

# Extract coefficients from lasso model
lasso_coefficients <- coef(lasso_model, s = best_lambda)
coeff_df <- as.data.frame(as.matrix(lasso_coefficients[[1]]))
coeff_df$Feature <- rownames(coeff_df)
colnames(coeff_df) <- c("Coefficient", "Feature")
coeff_df <- coeff_df[coeff_df$Coefficient != 0, ]  # Exclude zero coefficients

# Plot feature importance
ggplot(data = coeff_df, aes(x = reorder(Feature, Coefficient), y = Coefficient)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Feature Importance (Lasso Coefficients)", x = "Feature", y = "Coefficient") +
  theme_minimal()

```

## Class distribution of data set

```{r}
library(ggplot2)

class_counts <- as.data.frame(table(y))
colnames(class_counts) <- c("Class", "Count")

# Plot
ggplot(class_counts, aes(x = Class, y = Count, fill = Class)) +
  geom_bar(stat = "identity") +
  labs(title = "Class Distribution", x = "Class", y = "Count") +
  theme_minimal() +
  scale_fill_brewer(palette = "Set2")

```

